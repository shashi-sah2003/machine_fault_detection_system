{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashi-sah2003/machine_fault_detection_system/blob/main/machine_fault_detection_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Na8qA6hCaOR"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "skT61TgxLBZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-Da2ciFDHj29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/id_00.zip"
      ],
      "metadata": {
        "id": "tqUBPHYeI4VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d senaca/mimii-pump-sound-dataset"
      ],
      "metadata": {
        "id": "POt8YeQrC_P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow tensorflow-io matplotlib"
      ],
      "metadata": {
        "id": "Ly9mzZ8yDGmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from glob import glob\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from itertools import cycle\n",
        "\n",
        "sns.set_theme(style='white',palette=None)\n",
        "color_pal = plt.rcParams['axes.prop_cycle'].by_key()[\"color\"]\n",
        "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n"
      ],
      "metadata": {
        "id": "U7IhmMykDL7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q mimii-pump-sound-dataset.zip"
      ],
      "metadata": {
        "id": "wtwpvPZGDOPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abnormal_audio_files = glob(\"/content/id_00/abnormal/*.wav\")\n",
        "\n",
        "normal_audio_files=glob(\"/content/id_00/normal/*.wav\")"
      ],
      "metadata": {
        "id": "bKX2r3stDPxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_audio_files=normal_audio_files[:500]\n",
        "\n",
        "print(len(normal_audio_files))\n",
        "print(len(abnormal_audio_files))"
      ],
      "metadata": {
        "id": "_GWvczt1DYSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wavelet\n",
        "import pywt\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# Define the wavelet parameters\n",
        "wavelet = 'db4'  # Choose a specific wavelet (e.g., Daubechies 4)\n",
        "level = 5  # Level of decomposition\n",
        "\n",
        "normal_audio_wavelet = []\n",
        "for i in range(len(normal_audio_files)):\n",
        "    normal_y_wav, sr = librosa.load(normal_audio_files[i])\n",
        "    coeffs = pywt.wavedec(normal_y_wav, wavelet, level=level)\n",
        "    feature = np.concatenate(coeffs)\n",
        "    normal_audio_wavelet.append(feature)\n",
        "normal_audio_wavelet = np.array(normal_audio_wavelet)\n",
        "print(normal_audio_wavelet.shape)\n",
        "#print(normal_audio_wavelet)\n",
        "\n",
        "abnormal_audio_wavelet = []\n",
        "for i in range(len(abnormal_audio_files)):\n",
        "    abnormal_y_wav, sr = librosa.load(abnormal_audio_files[i])\n",
        "    coeffs = pywt.wavedec(abnormal_y_wav, wavelet, level=level)\n",
        "    feature = np.concatenate(coeffs)\n",
        "    abnormal_audio_wavelet.append(feature)\n",
        "abnormal_audio_wavelet = np.array(abnormal_audio_wavelet)\n",
        "print(abnormal_audio_wavelet.shape)\n",
        "#print(abnormal_audio_wavelet)\n"
      ],
      "metadata": {
        "id": "javFZBd7Dd_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kurtosis\n",
        "import numpy as np\n",
        "from scipy.stats import kurtosis\n",
        "normal_audio_wavelet_kurtosis = []\n",
        "for i in range(normal_audio_wavelet.shape[0]):\n",
        "    #print(normal_audio_wavelet[i].shape)\n",
        "    S = librosa.stft(normal_audio_wavelet[i])\n",
        "    sk = kurtosis(np.abs(S), axis=1)\n",
        "\n",
        "    # sk = kurtosis(normal_audio_wavelet[i])\n",
        "    #print(sk)\n",
        "    normal_audio_wavelet_kurtosis.append(sk)\n",
        "\n",
        "abnormal_audio_wavelet_kurtosis = []\n",
        "\n",
        "for i in range(abnormal_audio_wavelet.shape[0]):\n",
        "\n",
        "\n",
        "    S = librosa.stft(abnormal_audio_wavelet[i])\n",
        "    sk = kurtosis(np.abs(S), axis=1)\n",
        "\n",
        "    # sk = kurtosis(normal_audio_wavelet[i])\n",
        "    #print(sk)\n",
        "    abnormal_audio_wavelet_kurtosis.append(sk)\n",
        "print(normal_audio_wavelet)\n",
        "#print(abnormal_audio_wavelet.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qr4MJIDhDw58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_audio_wavelet_kurtosis= np.array(normal_audio_wavelet_kurtosis)\n",
        "print(normal_audio_wavelet_kurtosis.shape)\n",
        "abnormal_audio_wavelet_kurtosis= np.array(abnormal_audio_wavelet_kurtosis)\n",
        "print(abnormal_audio_wavelet_kurtosis.shape)\n"
      ],
      "metadata": {
        "id": "pzZ3jAVuD0Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spectral centroid\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "normal_audio_wavelet_centroid = []\n",
        "for i in range(len(normal_audio_wavelet)):\n",
        "    S = librosa.stft(normal_audio_wavelet[i])\n",
        "    centroid = librosa.feature.spectral_centroid(S=np.abs(S))\n",
        "    normal_audio_wavelet_centroid.append(centroid)\n",
        "normal_audio_wavelet_centroid = np.array(normal_audio_wavelet_centroid)\n",
        "print(normal_audio_wavelet_centroid.shape)\n",
        "print(normal_audio_wavelet_centroid)\n",
        "\n",
        "abnormal_audio_wavelet_centroid = []\n",
        "for i in range(len(abnormal_audio_wavelet)):\n",
        "    S = librosa.stft(abnormal_audio_wavelet[i])\n",
        "    centroid = librosa.feature.spectral_centroid(S=np.abs(S))\n",
        "    abnormal_audio_wavelet_centroid.append(centroid)\n",
        "abnormal_audio_wavelet_centroid = np.array(abnormal_audio_wavelet_centroid)\n",
        "print(abnormal_audio_wavelet_centroid.shape)\n",
        "print(abnormal_audio_wavelet_centroid)\n"
      ],
      "metadata": {
        "id": "HgtcdSLTwSpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_audio_wavelet_centroid = np.reshape(normal_audio_wavelet_centroid, (normal_audio_wavelet_centroid.shape[0], -1))\n",
        "abnormal_audio_wavelet_centroid = np.reshape(abnormal_audio_wavelet_centroid, (abnormal_audio_wavelet_centroid.shape[0], -1))\n",
        "print(normal_audio_wavelet_centroid.shape)"
      ],
      "metadata": {
        "id": "jfxWT7A8xdFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72735986-e24a-466c-eee4-cc779247f4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250, 431)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ZERO CROSSING RATE\n",
        "\n",
        "normal_audio_wavelet_zcr = []\n",
        "for i in range(len(normal_audio_wavelet)):\n",
        "    # normal_y_zcr, sr = librosa.load(normal_audio_wavelet[i])\n",
        "    zcr = librosa.feature.zero_crossing_rate(normal_audio_wavelet[i])\n",
        "    normal_audio_wavelet_zcr.append(zcr)\n",
        "normal_audio_wavelet_zcr = np.array(normal_audio_wavelet_zcr)\n",
        "print(normal_audio_wavelet_zcr.shape)\n",
        "#print(normal_audio_wavelet_zcr)\n",
        "\n",
        "abnormal_audio_wavelet_zcr = []\n",
        "for i in range(len(abnormal_audio_wavelet)):\n",
        "    #y, sr = librosa.load(abnormal_audio_wavelet[i])\n",
        "    zcr = librosa.feature.zero_crossing_rate(abnormal_audio_wavelet[i])\n",
        "    abnormal_audio_wavelet_zcr.append(zcr)\n",
        "abnormal_audio_wavelet_zcr = np.array(abnormal_audio_wavelet_zcr)\n",
        "print(abnormal_audio_wavelet_zcr.shape)\n",
        "print(abnormal_audio_wavelet_zcr)"
      ],
      "metadata": {
        "id": "eV8faDB4EQfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MEL Spectrogram\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "\n",
        "normal_wav_audio_melspec = []\n",
        "for audio_data in normal_audio_wavelet:\n",
        "    melspec = librosa.feature.melspectrogram(y=audio_data, sr=sr)\n",
        "    normal_wav_audio_melspec.append(melspec)\n",
        "normal_wav_audio_melspec = np.array(normal_wav_audio_melspec)\n",
        "#print(abnormal_wav_audio_melspec.shape)\n",
        "print(normal_wav_audio_melspec)\n",
        "\n",
        "abnormal_wav_audio_melspec = []\n",
        "for audio_data in abnormal_audio_wavelet:\n",
        "    melspec = librosa.feature.melspectrogram(y=audio_data, sr=sr)\n",
        "    abnormal_wav_audio_melspec.append(melspec)\n",
        "abnormal_wav_audio_melspec = np.array(abnormal_wav_audio_melspec)\n",
        "#print(abnormal_wav_audio_melspec.shape)\n",
        "print(abnormal_wav_audio_melspec)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "evb7bgtpW29O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_wav_audio_melspec = np.reshape(normal_wav_audio_melspec, (normal_wav_audio_melspec.shape[0], -1))\n",
        "abnormal_wav_audio_melspec = np.reshape(abnormal_wav_audio_melspec, (abnormal_wav_audio_melspec.shape[0], -1))"
      ],
      "metadata": {
        "id": "fxsOhIw9K3RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j_6DsLJFr6Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(abnormal_wav_audio_melspec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biAKMYT3LT-Q",
        "outputId": "b54fc2fe-9b1b-42be-e954-61da6b0ca6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(143, 55168)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normal_contac_all=[]\n",
        "for i in range(len(normal_audio_wavelet_kurtosis)):\n",
        "  contac= np.concatenate((normal_audio_wavelet_kurtosis[i],normal_audio_wavelet_zcr[i][0],normal_wav_audio_melspec[i],normal_audio_wavelet_centroid[i]))\n",
        "  normal_contac_all.append(contac)\n",
        "normal_contac_all=np.array(normal_contac_all)\n",
        "\n",
        "abnormal_contac_all=[]\n",
        "for i in range(len(abnormal_audio_wavelet_kurtosis)):\n",
        "  ab_contac= np.concatenate((abnormal_audio_wavelet_kurtosis[i],abnormal_audio_wavelet_zcr[i][0],abnormal_wav_audio_melspec[i],abnormal_audio_wavelet_centroid[i]))\n",
        "  abnormal_contac_all.append(ab_contac)\n",
        "abnormal_contac_all=np.array(abnormal_contac_all)"
      ],
      "metadata": {
        "id": "ihKqt8gzFj-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_contac_all.shape)\n",
        "print(abnormal_contac_all.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlTWcz5mGMS9",
        "outputId": "a6b4e251-d416-47f5-8ba7-bde0c2ab516b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250, 57055)\n",
            "(143, 57055)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y1=np.zeros(normal_contac_all.shape[0])\n",
        "y2=np.ones(abnormal_contac_all.shape[0])\n",
        "y_features=np.concatenate((y1, y2))\n",
        "# print(y_train)\n",
        "\n",
        "X_features=np.concatenate((normal_contac_all,abnormal_contac_all ))\n",
        "print(y_features.shape)\n",
        "print(X_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTjpMAVeGgc_",
        "outputId": "dc698de6-8b21-46aa-9f00-867e3b2905e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(393,)\n",
            "(393, 57055)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a NumPy array\n",
        "data = X_features\n",
        "labels=y_features\n",
        "\n",
        "# Create a NumPy array of labels\n",
        "# labels = np.array([0, 1, 0, 1])\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Shuffle the indices\n",
        "indices = np.random.permutation(len(data))\n",
        "\n",
        "# Shuffle the data and labels based on the shuffled indices\n",
        "shuffled_data = data[indices]\n",
        "shuffled_labels = labels[indices]\n",
        "\n",
        "# Set the desired ratio for train-test split\n",
        "train_ratio = 0.75\n",
        "\n",
        "# Calculate the split index\n",
        "split_index = int(train_ratio * len(data))\n",
        "\n",
        "# Split the data and labels into train and test sets\n",
        "train_data_features = shuffled_data[:split_index]\n",
        "train_labels_features = shuffled_labels[:split_index]\n",
        "test_data_features = shuffled_data[split_index:]\n",
        "test_labels_features = shuffled_labels[split_index:]\n",
        "\n",
        "# Print the train and test sets\n",
        "print(\"Training Data:\")\n",
        "print(train_data_features)\n",
        "print(\"Training Labels:\")\n",
        "print(train_labels_features)\n",
        "print(\"Testing Data:\")\n",
        "print(test_data_features)\n",
        "print(\"Testing Labels:\")\n",
        "print(test_labels_features)"
      ],
      "metadata": {
        "id": "1SN2lZ2QGoez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "#np.random.seed(42)\n",
        "\n",
        "# Split the data and labels into train and test sets\n",
        "train_data_features, test_data_features, train_labels_features, test_labels_features = train_test_split(\n",
        "    data, labels, train_size=0.75, random_state=0)\n",
        "\n",
        "# Print the train and test sets\n",
        "print(\"Training Data:\")\n",
        "print(train_data_features)\n",
        "print(\"Training Labels:\")\n",
        "print(train_labels_features)\n",
        "print(\"Testing Data:\")\n",
        "print(test_data_features)\n",
        "print(\"Testing Labels:\")\n",
        "print(test_labels_features)\n"
      ],
      "metadata": {
        "id": "RLJ2EwAohAjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "#test_data = test_data_features\n",
        "test_labels = test_labels_features"
      ],
      "metadata": {
        "id": "7AJ70JUIEM6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "train_data = scaler.fit_transform(train_data_features)\n",
        "test_data = scaler.transform(test_data_features)"
      ],
      "metadata": {
        "id": "Hf8m8RBeEMq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensemble model of Random forest and XGboost\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "xgboost = XGBClassifier()\n",
        "\n",
        "ensemble_model = VotingClassifier(estimators=[('rf', random_forest), ('xgb', xgboost)], voting='hard')\n",
        "ensemble_model.fit(train_data, train_labels)\n",
        "ensemble_model.predict(test_data)\n",
        "ensemble_model.score(test_data, test_labels)\n"
      ],
      "metadata": {
        "id": "xrdSt1vbjiN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have already trained and tested the ensemble_model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ensemble_model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, y_pred)\n",
        "\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "b-5N5Ezcp8Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensemble model of RFC and SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "svc = SVC(probability=True)  # Set probability=True for soft voting\n",
        "\n",
        "ensemble_model = VotingClassifier(estimators=[('rf', random_forest), ('svc', svc)], voting='soft')\n",
        "\n",
        "ensemble_model.fit(train_data, train_labels)\n",
        "ensemble_model.predict(test_data)\n",
        "ensemble_model.score(test_data, test_labels)\n"
      ],
      "metadata": {
        "id": "vGHSX62Sozv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have already trained and tested the ensemble_model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ensemble_model.predict(test_data_scaled)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, y_pred)\n",
        "\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "NHp8QYLxqqbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensemble model of SVC and XGboost\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "svc = SVC(probability=True)  # Set probability=True for soft voting\n",
        "xgboost = XGBClassifier()\n",
        "\n",
        "ensemble_model = VotingClassifier(estimators=[('svc', svc), ('xgb', xgboost)], voting='soft')\n",
        "\n",
        "ensemble_model.fit(train_data, train_labels)\n",
        "ensemble_model.predict(test_data)\n",
        "ensemble_model.score(test_data, test_labels)\n"
      ],
      "metadata": {
        "id": "U5VsOKPusM_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have already trained and tested the ensemble_model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ensemble_model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, y_pred)\n",
        "\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "hvRDb0Lft88Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# # Generate dummy data\n",
        "# data = np.random.random((1000, 10))  # Replace with your own data\n",
        "# labels = np.random.randint(2, size=(1000, 1))  # Replace with your own labels\n",
        "\n",
        "# Split data into training and testing sets\n",
        "#train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "#test_data = test_data_features\n",
        "test_labels = test_labels_features\n",
        "\n",
        "# Build the ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_dim=train_data.shape[1], activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Set the learning rate\n",
        "# learning_rate = 0.01\n",
        "# optimizer = Adam(learning_rate=learning_rate)\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, epochs=50, batch_size=16, verbose=1)\n",
        "# model.fit(train_data, train_labels, verbose=1)\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_data, test_labels, verbose=1)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "rf8emypKGr3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "mNoLsipWykWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import SVC\n",
        "\n",
        "# # Split data into training and testing sets\n",
        "# train_data = train_data_features\n",
        "# train_labels = train_labels_features\n",
        "# test_data = test_data_features\n",
        "# test_labels = test_labels_features\n",
        "\n",
        "# # Build the SVM model\n",
        "# model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(train_data, train_labels)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = model.score(test_data, test_labels)\n",
        "# print('Test Accuracy:', accuracy)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # Normalize the data\n",
        "# scaler = StandardScaler()\n",
        "# train_data_scaled = scaler.fit_transform(train_data_features)\n",
        "# test_data_scaled = scaler.transform(test_data_features)\n",
        "\n",
        "# Build the SVM model\n",
        "model = SVC()\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'gamma': ['scale', 'auto'],\n",
        "              'kernel': ['linear', 'rbf']}\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(train_data, train_labels_features)\n",
        "\n",
        "# Get the best model from grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Perform cross-validation to evaluate the model\n",
        "cross_val_scores = cross_val_score(best_model, train_data, train_labels_features, cv=5)\n",
        "mean_cross_val_accuracy = cross_val_scores.mean()\n",
        "\n",
        "# Train the best model on the entire training data\n",
        "best_model.fit(train_data, train_labels_features)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_accuracy = best_model.score(test_data, test_labels_features)\n",
        "\n",
        "print('Cross-Validation Accuracy:', mean_cross_val_accuracy)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "print('Best Model:', best_model)\n",
        "\n"
      ],
      "metadata": {
        "id": "cnwxGIB2GyrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = best_model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = best_model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "rgEwaPWTy67i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# # Generate dummy data\n",
        "# data = np.random.random((1000, 10))  # Replace with your own data\n",
        "# labels = np.random.randint(2, size=(1000, 1))  # Replace with your own labels\n",
        "\n",
        "# # Split data into training and testing sets\n",
        "train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "test_data = test_data_features\n",
        "test_labels = test_labels_features\n",
        "\n",
        "# Reshape data to fit LSTM input shape\n",
        "train_data = np.reshape(train_data, (train_data.shape[0], 1, train_data.shape[1]))\n",
        "test_data = np.reshape(test_data, (test_data.shape[0], 1, test_data.shape[1]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(1, train_data.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, epochs=25, batch_size=16, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_data, test_labels, verbose=1)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "kEzdoYUPJSHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "1oOoK3tnzEjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split data into training and testing sets\n",
        "#train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "#test_data = test_data_features\n",
        "test_labels = test_labels_features\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "model = RandomForestClassifier(n_estimators=60)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "-LCIwpgKJv88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f21e02-6702-403d-badc-6ba519be5b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9393939393939394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n"
      ],
      "metadata": {
        "id": "0HMn9jv2zG3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have the training and testing data and labels\n",
        "# train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "# test_data = test_data_features\n",
        "test_labels = test_labels_features\n",
        "\n",
        "# Create the XGBoost Classifier model\n",
        "model = xgb.XGBClassifier()\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "7RoZ5_WotJ13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "X3L4_TuF0ngh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JT9j1KOe6BAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}